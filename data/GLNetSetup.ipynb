{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GLNetSetup.ipynb",
      "provenance": [],
<<<<<<< HEAD
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI5AgyQBthAE2azY5BbeJU",
      "include_colab_link": true
=======
      "collapsed_sections": []
>>>>>>> origin
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1553e4b5eba8401183c0255d34d5af00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_989a7ab04d1e48289595c20630ba6689",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b894aa3bcb914c7ab24a7e7345841482",
              "IPY_MODEL_2f8d189cd2bb4082a061bb41af62bc67"
            ]
          }
        },
        "989a7ab04d1e48289595c20630ba6689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b894aa3bcb914c7ab24a7e7345841482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d79d077d870d4b30823feb2435a2260c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_969c94624bfc444b917c21fe4f3122a4"
          }
        },
        "2f8d189cd2bb4082a061bb41af62bc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_85812034d8fc40268f37c93a55c5d95b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:16&lt;00:00, 6.14MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d2c96b0061c4f7e9c97cd35fa32aadd"
          }
        },
        "d79d077d870d4b30823feb2435a2260c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "969c94624bfc444b917c21fe4f3122a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85812034d8fc40268f37c93a55c5d95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d2c96b0061c4f7e9c97cd35fa32aadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Orienfish/robust-lora/blob/emily_refactoring/GLNetSetup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L-ma3EZMjMX"
      },
      "source": [
        "## Mount GLNet and install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbv2InXyy2I1",
<<<<<<< HEAD
        "outputId": "13ec1038-c16a-4929-8e84-5a71df063a99"
=======
        "outputId": "0593c8b6-1c9d-40ef-bbf7-ce9fad3bcd1a"
>>>>>>> origin
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
<<<<<<< HEAD
        "%cd drive/MyDrive/GLNet-master\n",
=======
        "%cd drive/MyDrive/GLNet\n",
>>>>>>> origin
        "%pip install -r requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
<<<<<<< HEAD
            "/content/drive/MyDrive/GLNet-master\n",
=======
            "/content/drive/MyDrive/GLNet\n",
>>>>>>> origin
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.1)\n",
            "Requirement already satisfied: Pillow==6.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (6.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.1.2.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 2)) (0.8)\n",
<<<<<<< HEAD
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 5)) (51.3.3)\n"
=======
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 5)) (53.0.0)\n"
>>>>>>> origin
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzpfZYkSNaZx"
      },
      "source": [
        "## Configure GLNet to be ready for the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hR5G14kREa",
        "outputId": "6337476b-0991-4084-ffe5-25715da93a90"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from dataset.deep_globe import DeepGlobe, classToRGB, is_image_file\n",
        "from utils.loss import CrossEntropyLoss2d, SoftCrossEntropyLoss2d, FocalLoss\n",
        "from utils.lovasz_losses import lovasz_softmax\n",
        "from utils.lr_scheduler import LR_Scheduler\n",
        "\n",
        "from helper import create_model_load_weights, get_optimizer, Trainer, Evaluator, collate, collate_test\n",
        "from option import Options\n",
        "\n",
        "\n",
<<<<<<< HEAD
        "from PIL import Image, ImageDraw"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hR5G14kREa",
        "outputId": "d84e2587-6d5f-4a9d-f993-7d227bdc4d7e"
      },
      "source": [
=======
        "from PIL import Image, ImageDraw\n",
        "\n",
>>>>>>> origin
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "n_class = 7\n",
        "\n",
        "data_path = './data'\n",
        "if not os.path.isdir(data_path): os.mkdir(data_path)\n",
        "\n",
        "# added the creation of test and Sat folder to prevent future errors\n",
        "test_path = './data/test'\n",
        "if not os.path.isdir(test_path): os.mkdir(test_path)\n",
        "\n",
        "test_Sat_path = './data/test/Sat'\n",
        "if not os.path.isdir(test_Sat_path): os.mkdir(test_Sat_path)\n",
        "\n",
        "model_path = './saved_models'\n",
        "if not os.path.isdir(model_path): os.mkdir(model_path)\n",
        "\n",
        "log_path = './runs'\n",
        "if not os.path.isdir(log_path): os.mkdir(log_path)\n",
        "\n",
        "task_name = 'test'\n",
        "\n",
        "print(task_name)\n",
        "\n",
        "###################################\n",
        "\n",
        "# 1: train global; 2: train local from global; 3: train global from local\n",
        "mode = 3 \n",
        "evaluation = False\n",
        "test = not evaluation    \n",
        "print(\"mode:\", mode, \"evaluation:\", evaluation, \"test:\", test)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
<<<<<<< HEAD
        "###################################\n",
        "\n",
        "\n",
        "if_crop = True\n",
        "crop_image_name = 'test.jpg'\n",
        "sub_image_dim = 1600\n"
=======
        "###################################"
>>>>>>> origin
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "mode: 3 evaluation: False test: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrRs1zkylhh"
      },
      "source": [
        "## Prepare the datasets.\n",
        "* Make sure you have upload your image (e.g., 'la.jpg') into 'root-of-GLNet/data/test' and download the pretrained models to 'root-of-GLNet/saved_models' folder.\n",
        "* Specify the name of the image to be tested in the second line of the next block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFDlwR5LlMfR",
<<<<<<< HEAD
        "outputId": "062beb55-d630-462b-9318-916c5e0c92ad"
=======
        "outputId": "f92237c0-2e80-43e1-f367-cfd064275be2"
>>>>>>> origin
      },
      "source": [
        "if_crop = True\n",
        "crop_image_name = 'la.jpg' # Specify the image you want to test here\n",
        "sub_image_dim = 1600\n",
        "\n",
        "print(\"preparing datasets and dataloaders......\")\n",
        "\n",
        "\n",
        "def pad_image(pil_img, new_width, new_height, color):\n",
        "    width, height = pil_img.size\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (0, 0))\n",
        "    return result\n",
        "\n",
        "def crop_image(pil_img, h_num, v_num, dim):  \n",
        "    for i in range(v_num):\n",
        "        for j in range(h_num):\n",
        "            box = (j*dim, i*dim, j*dim+dim, i*dim+dim)\n",
        "            sub_image = pil_img.crop(box).save(os.path.join(data_path, \"test/Sat/\" + 'crop_img_{}_{}.jpg'.format(i,j)), quality=95)\n",
        "            \n",
        "            \n",
        "####### Image Cropping #########\n",
        "if if_crop:\n",
        "    original_image = Image.open(os.path.join(data_path, \"test/\" + crop_image_name))\n",
        "\n",
        "    h_num = int(original_image.size[0]/sub_image_dim+0.5)\n",
        "    v_num = int(original_image.size[1]/sub_image_dim+0.5)\n",
        "\n",
        "    new_width = h_num*sub_image_dim\n",
        "    new_height = v_num*sub_image_dim\n",
        "    new_pad_iamge = pad_image(original_image, new_width, new_height, (0, 0, 0))\n",
        "    new_pad_iamge.save(os.path.join(data_path, \"test/\" + 'pad_image.jpg'), quality=95)\n",
        "\n",
        "    crop_image(new_pad_iamge, h_num, v_num, dim=sub_image_dim)\n",
        "\n",
        "\n",
        "image_name_test = [image_name for image_name in os.listdir(os.path.join(data_path, \"test\", \"Sat\")) if is_image_file(image_name)]\n",
        "print(image_name_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing datasets and dataloaders......\n",
            "['crop_img_1_1.jpg', 'crop_img_0_0.jpg', 'crop_img_0_2.jpg', 'crop_img_1_2.jpg', 'crop_img_0_1.jpg', 'crop_img_1_0.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1UDCqBrOsVh"
      },
      "source": [
        "## Start segmentng the image using GLNet!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "1553e4b5eba8401183c0255d34d5af00",
            "989a7ab04d1e48289595c20630ba6689",
            "b894aa3bcb914c7ab24a7e7345841482",
            "2f8d189cd2bb4082a061bb41af62bc67",
            "d79d077d870d4b30823feb2435a2260c",
            "969c94624bfc444b917c21fe4f3122a4",
            "85812034d8fc40268f37c93a55c5d95b",
            "5d2c96b0061c4f7e9c97cd35fa32aadd"
          ]
        },
        "id": "F6oxK6Lgme9F",
<<<<<<< HEAD
        "outputId": "5a2ea715-b08d-45fe-bea2-7f8ab5992d6e"
=======
        "outputId": "e26eeeef-f906-498a-9c95-e1c1a72e8892"
>>>>>>> origin
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset_test = DeepGlobe(os.path.join(data_path, \"test\"), image_name_test, label=False)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, num_workers=10, collate_fn=collate_test, shuffle=False, pin_memory=True)\n",
        "\n",
        "###################################\n",
        "\n",
        "size_g = 508\n",
        "size_p = 508\n",
        "\n",
        "##### sizes are (w, h) ##############################\n",
        "\n",
        "# make sure margin / 32 is over 1.5 AND size_g is divisible by 4\n",
        "size_g = (size_g, size_g) # resized global image\n",
        "size_p = (size_p, size_p) # cropped local patch size\n",
        "sub_batch_size = 6 # batch size for train local patches\n",
        "\n",
        "path_g  = \"fpn_deepglobe_global.pth\"\n",
        "path_g2l = \"fpn_deepglobe_global2local.pth\" \n",
        "path_l2g = \"fpn_deepglobe_local2global.pth\"\n",
        "\n",
        "###################################\n",
        "\n",
        "print(\"creating models......\")\n",
        "\n",
        "path_g = os.path.join(model_path, path_g)\n",
        "path_g2l = os.path.join(model_path, path_g2l)\n",
        "path_l2g = os.path.join(model_path, path_l2g)\n",
        "model, global_fixed = create_model_load_weights(n_class, mode, evaluation, path_g=path_g, path_g2l=path_g2l, path_l2g=path_l2g)\n",
        "\n",
        "##################################\n",
        "\n",
<<<<<<< HEAD
        "evaluator = Evaluator(n_class, size_g, size_p, sub_batch_size, mode, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating models......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2ZN7EeBmwqP",
        "outputId": "80342b53-cfbe-41e3-bbfe-fd89e76ff6b8"
      },
      "source": [
=======
        "evaluator = Evaluator(n_class, size_g, size_p, sub_batch_size, mode, test)\n",
        "\n",
>>>>>>> origin
        "best_pred = 0.0\n",
        "print(\"start testing......\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    print(\"testing...\")\n",
        "\n",
        "    img_count = 0\n",
        "    if if_crop:\n",
        "        entire_seg_map = np.zeros([v_num*sub_image_dim, h_num*sub_image_dim], dtype=np.int8)\n",
        "        \n",
        "    if test: tbar = tqdm(dataloader_test)\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(tbar):\n",
        "        # print(\"\\nTesting {}/{} image \\n\".format(img_count+1, len(image_name_test)) )\n",
        "        predictions, predictions_global, predictions_local = evaluator.eval_test(sample_batched, model, global_fixed)\n",
        "        score_val, score_val_global, score_val_local = evaluator.get_scores()\n",
        "\n",
        "        if mode == 1: tbar.set_description('global mIoU: %.3f' % (np.mean(np.nan_to_num(score_val_global[\"iou\"])[1:])))\n",
        "        else: tbar.set_description('agg mIoU: %.3f' % (np.mean(np.nan_to_num(score_val[\"iou\"])[1:])))\n",
        "        images = sample_batched['image']\n",
        "\n",
        "        # print(images)\n",
        "\n",
        "        if test:\n",
        "            if not os.path.isdir(\"./prediction/\"): os.mkdir(\"./prediction/\")\n",
        "\n",
        "            for i in range(len(images)):\n",
        "                if mode == 1:\n",
        "                    transforms.functional.to_pil_image(classToRGB(predictions_global[i]) * 255.).save(\"./prediction/\" + 'sat_test_{}_mask.png'.format(img_count))\n",
        "                    img_count = img_count+1\n",
        "                    pred_seg_map = predictions_global[0]\n",
        "                else:\n",
        "                    transforms.functional.to_pil_image(classToRGB(predictions[i]) * 255.).save(\"./prediction/\" + 'sat_test_{}_mask.png'.format(img_count))\n",
        "                    img_count = img_count+1\n",
        "                    pred_seg_map = predictions[0]\n",
        "      \n",
        "\n",
        "            if if_crop:\n",
        "                left_index = ((img_count-1) % h_num)*sub_image_dim\n",
        "                up_index = ((img_count-1)//h_num)*sub_image_dim\n",
        "                entire_seg_map[up_index:up_index+sub_image_dim, left_index:left_index+sub_image_dim ] = pred_seg_map\n",
        "\n",
        "seg_map_save_path = \"./prediction/\" + 'sat_test_{}_mask.png'.format('entire')\n",
        "transforms.functional.to_pil_image(classToRGB(entire_seg_map) * 255.).save(seg_map_save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating models......\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1553e4b5eba8401183c0255d34d5af00",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start testing......\n",
            "testing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/drive/My Drive/GLNet/utils/metrics.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
            "  acc = np.nan_to_num(np.diag(hist) / hist.sum(axis=1))\n",
            "/content/drive/My Drive/GLNet/utils/metrics.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
            "  iou = intersect / union\n",
            "/content/drive/My Drive/GLNet/utils/metrics.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
            "  freq = hist.sum(axis=1) / hist.sum() # freq of each target\n",
<<<<<<< HEAD
            "agg mIoU: 0.000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:15<00:00, 22.62s/it]\n"
=======
            "agg mIoU: 0.000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:13<00:00, 22.29s/it]\n"
>>>>>>> origin
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWVb6-_GO7Q3"
      },
      "source": [
        "## Convert image into np array and save as npy file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ_5lm4MX9yu",
        "outputId": "22c1f66f-d7ad-4305-a27f-80f3685a3ecf"
=======
        "id": "MJ_5lm4MX9yu"
>>>>>>> origin
      },
      "source": [
        "from PIL import Image \n",
        "import numpy as np\n",
        "\n",
        "# open image in working directory\n",
        "image = Image.open(\"./prediction/sat_test_entire_mask.png\")\n",
        "orgimage = Image.open(\"./data/test/test.jpg\")\n",
        "width, height = orgimage.size\n",
        "\n",
        "#use original image dimensions to crop new image\n",
        "im1 = image.crop((0, 0, width, height)) \n",
        "im1.save('cropped_result', 'PNG')\n",
        "\n",
        "# convert image into np array and save as npy file\n",
<<<<<<< HEAD
        "data = np.array(im1)\n",
        "print(data)\n",
        "np.save('entire_seg_map.npy', data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]]\n",
            "\n",
            " [[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]]\n",
            "\n",
            " [[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]]\n",
            "\n",
            " [[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]]\n",
            "\n",
            " [[  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  ...\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]\n",
            "  [  0   0 255]]]\n"
          ],
          "name": "stdout"
        }
      ]
=======
        "data = np.array(image)\n",
        "# print(data)\n",
        "np.save('entire_seg_map.npy', data)"
      ],
      "execution_count": null,
      "outputs": []
>>>>>>> origin
    }
  ]
}